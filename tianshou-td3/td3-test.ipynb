{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a264276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import tianshou as ts\n",
    "from tianshou.algorithm.modelfree.ddpg import (\n",
    "    ContinuousDeterministicPolicy,\n",
    "    ContinuousPolicyWithExplorationNoise,\n",
    ")\n",
    "from tianshou.algorithm.modelfree.td3 import TD3\n",
    "from tianshou.algorithm.optim import AdamOptimizerFactory\n",
    "from tianshou.data import CollectStats, Collector\n",
    "from tianshou.trainer import OffPolicyTrainerParams\n",
    "from tianshou.utils.net.common import Net\n",
    "from tianshou.utils.net.continuous import ContinuousActorDeterministic, ContinuousCritic\n",
    "from tianshou.utils.space_info import SpaceInfo\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.exploration.random import GaussianNoise\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d72660bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add continuous environments\n",
    "# import bbrl_utils\n",
    "# import bbrl_gymnasium  # noqa: F401\n",
    "\n",
    "# bbrl_utils.setup()\n",
    "\n",
    "# assert \"CartPoleContinuous-v1\" in gym.envs.registry.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0725dc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['CartPole-v0', 'CartPole-v1', 'MountainCar-v0', 'MountainCarContinuous-v0', 'Pendulum-v1', 'Acrobot-v1', 'phys2d/CartPole-v0', 'phys2d/CartPole-v1', 'phys2d/Pendulum-v0', 'LunarLander-v3', 'LunarLanderContinuous-v3', 'BipedalWalker-v3', 'BipedalWalkerHardcore-v3', 'CarRacing-v3', 'Blackjack-v1', 'FrozenLake-v1', 'FrozenLake8x8-v1', 'CliffWalking-v1', 'CliffWalkingSlippery-v1', 'Taxi-v3', 'tabular/Blackjack-v0', 'tabular/CliffWalking-v0', 'Reacher-v2', 'Reacher-v4', 'Reacher-v5', 'Pusher-v2', 'Pusher-v4', 'Pusher-v5', 'InvertedPendulum-v2', 'InvertedPendulum-v4', 'InvertedPendulum-v5', 'InvertedDoublePendulum-v2', 'InvertedDoublePendulum-v4', 'InvertedDoublePendulum-v5', 'HalfCheetah-v2', 'HalfCheetah-v3', 'HalfCheetah-v4', 'HalfCheetah-v5', 'Hopper-v2', 'Hopper-v3', 'Hopper-v4', 'Hopper-v5', 'Swimmer-v2', 'Swimmer-v3', 'Swimmer-v4', 'Swimmer-v5', 'Walker2d-v2', 'Walker2d-v3', 'Walker2d-v4', 'Walker2d-v5', 'Ant-v2', 'Ant-v3', 'Ant-v4', 'Ant-v5', 'Humanoid-v2', 'Humanoid-v3', 'Humanoid-v4', 'Humanoid-v5', 'HumanoidStandup-v2', 'HumanoidStandup-v4', 'HumanoidStandup-v5', 'GymV21Environment-v0', 'GymV26Environment-v0'])\n"
     ]
    }
   ],
   "source": [
    "print(gym.envs.registry.keys())\n",
    "# env = gym.make(\"MountainCarContinuous-v0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70f81e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task = \"CartPoleContinuous-v1\"\n",
    "task = \"Pendulum-v1\"\n",
    "lr_actor, epoch, batch_size = 1e-3, 10, 64\n",
    "lr_critic = 1e-3\n",
    "num_training_envs, num_testing_envs = 5, 50\n",
    "gamma = 0.99\n",
    "tau = 0.005\n",
    "buffer_size = 100_000\n",
    "epoch_num_steps, collection_step_num_env_steps = 5000, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c10b8fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ActionToScalar(gym.ActionWrapper):\n",
    "#     def action(self, act):\n",
    "#         # act is usually shape (1,) -> convert to a python float\n",
    "#         return float(np.asarray(act).reshape(-1)[0])\n",
    "\n",
    "\n",
    "# def make_env(task):\n",
    "#     env = gym.make(task, disable_env_checker=True)\n",
    "#     env = ActionToScalar(env)\n",
    "#     return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dab14459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "env = gym.make(task)\n",
    "space_info = SpaceInfo.from_env(env)\n",
    "state_shape = space_info.observation_info.obs_shape\n",
    "action_shape = space_info.action_info.action_shape\n",
    "\n",
    "logger = ts.utils.TensorboardLogger(SummaryWriter(\"log/td3\"))\n",
    "\n",
    "training_envs = DummyVectorEnv(\n",
    "    [lambda: gym.make(task) for _ in range(num_training_envs)]\n",
    ")\n",
    "\n",
    "test_envs = DummyVectorEnv([lambda: gym.make(task) for _ in range(num_testing_envs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "baa4c4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial test step: test_reward: -1273.497396 ± 295.436084, best_reward: -1273.497396 ± 295.436084 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 100%|##########| 5000/5000 [00:14<00:00, 335.16it/s, env_episode=25, env_step=5000, len=200, n_ep=5, n_st=5, rew=-610.13, update_step=1000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: -243.227987 ± 250.064336, best_reward: -243.227987 ± 250.064336 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 100%|##########| 5000/5000 [00:14<00:00, 354.82it/s, env_episode=50, env_step=10000, len=200, n_ep=5, n_st=5, rew=-192.48, update_step=2000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: -148.954922 ± 83.290718, best_reward: -148.954922 ± 83.290718 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #3: 100%|##########| 5000/5000 [00:14<00:00, 353.23it/s, env_episode=75, env_step=15000, len=200, n_ep=5, n_st=5, rew=-196.54, update_step=3000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3: test_reward: -169.860997 ± 94.279373, best_reward: -148.954922 ± 83.290718 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #4: 100%|##########| 5000/5000 [00:14<00:00, 347.37it/s, env_episode=100, env_step=20000, len=200, n_ep=5, n_st=5, rew=-173.96, update_step=4000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4: test_reward: -145.042790 ± 77.540787, best_reward: -145.042790 ± 77.540787 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #5: 100%|##########| 5000/5000 [00:14<00:00, 345.61it/s, env_episode=125, env_step=25000, len=200, n_ep=5, n_st=5, rew=-114.50, update_step=5000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5: test_reward: -127.696114 ± 80.982816, best_reward: -127.696114 ± 80.982816 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #6: 100%|##########| 5000/5000 [00:14<00:00, 344.09it/s, env_episode=150, env_step=30000, len=200, n_ep=5, n_st=5, rew=-100.79, update_step=6000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #6: test_reward: -150.004917 ± 70.868086, best_reward: -127.696114 ± 80.982816 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #7: 100%|##########| 5000/5000 [00:14<00:00, 334.89it/s, env_episode=175, env_step=35000, len=200, n_ep=5, n_st=5, rew=-191.50, update_step=7000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #7: test_reward: -152.278776 ± 63.310081, best_reward: -127.696114 ± 80.982816 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #8: 100%|##########| 5000/5000 [00:15<00:00, 316.57it/s, env_episode=200, env_step=40000, len=200, n_ep=5, n_st=5, rew=-150.70, update_step=8000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #8: test_reward: -167.232848 ± 78.215232, best_reward: -127.696114 ± 80.982816 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #9: 100%|##########| 5000/5000 [00:15<00:00, 320.37it/s, env_episode=225, env_step=45000, len=200, n_ep=5, n_st=5, rew=-149.97, update_step=9000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #9: test_reward: -139.631572 ± 73.820023, best_reward: -127.696114 ± 80.982816 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #10: 100%|##########| 5000/5000 [00:15<00:00, 315.88it/s, env_episode=250, env_step=50000, len=200, n_ep=5, n_st=5, rew=-365.04, update_step=10000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #10: test_reward: -146.020494 ± 89.321157, best_reward: -127.696114 ± 80.982816 in #5\n"
     ]
    }
   ],
   "source": [
    "# Actor\n",
    "net_actor = Net(\n",
    "    state_shape=state_shape, action_shape=action_shape, hidden_sizes=[400, 300]\n",
    ")\n",
    "actor = ContinuousActorDeterministic(\n",
    "    preprocess_net=net_actor, action_shape=action_shape\n",
    ")\n",
    "actor_optim = AdamOptimizerFactory(lr=lr_actor)\n",
    "\n",
    "# Critics\n",
    "net_critic_1 = Net(\n",
    "    state_shape=state_shape,\n",
    "    action_shape=action_shape,\n",
    "    hidden_sizes=[400, 300],\n",
    "    concat=True,\n",
    ")\n",
    "critic_1 = ContinuousCritic(preprocess_net=net_critic_1)\n",
    "critic_1_optim = AdamOptimizerFactory(lr=lr_critic)\n",
    "\n",
    "net_critic_2 = Net(\n",
    "    state_shape=state_shape,\n",
    "    action_shape=action_shape,\n",
    "    hidden_sizes=[400, 300],\n",
    "    concat=True,\n",
    ")\n",
    "critic_2 = ContinuousCritic(preprocess_net=net_critic_2)\n",
    "critic_2_optim = AdamOptimizerFactory(lr=lr_critic)\n",
    "\n",
    "# Policy\n",
    "policy = ContinuousDeterministicPolicy(\n",
    "    actor=actor, action_space=env.action_space, exploration_noise=GaussianNoise(0, 0.5)\n",
    ")\n",
    "\n",
    "# Algorithm\n",
    "algorithm = TD3(\n",
    "    policy=policy,\n",
    "    policy_optim=actor_optim,\n",
    "    critic=critic_1,\n",
    "    critic_optim=critic_1_optim,\n",
    "    critic2=critic_2,\n",
    "    critic2_optim=critic_2_optim,\n",
    "    gamma=gamma,\n",
    ")\n",
    "\n",
    "# Collectors\n",
    "training_collector = Collector[CollectStats](\n",
    "    algorithm,\n",
    "    training_envs,\n",
    "    ts.data.VectorReplayBuffer(buffer_size, num_training_envs),\n",
    "    exploration_noise=True,\n",
    ")\n",
    "\n",
    "test_collector = Collector[CollectStats](algorithm, test_envs, exploration_noise=False)\n",
    "\n",
    "\n",
    "def stop_fn(mean_rewards: float) -> bool:\n",
    "    if env.spec:\n",
    "        if not env.spec.reward_threshold:\n",
    "            return False\n",
    "        else:\n",
    "            return mean_rewards >= env.spec.reward_threshold\n",
    "    return False\n",
    "\n",
    "\n",
    "# Warm up\n",
    "training_collector.reset()\n",
    "training_collector.collect(n_step=10_000, random=True)\n",
    "\n",
    "off_policy_trainer_params = OffPolicyTrainerParams(\n",
    "    training_collector=training_collector,\n",
    "    test_collector=test_collector,\n",
    "    max_epochs=epoch,\n",
    "    epoch_num_steps=epoch_num_steps,\n",
    "    collection_step_num_env_steps=collection_step_num_env_steps,\n",
    "    test_step_num_episodes=num_testing_envs,\n",
    "    batch_size=batch_size,\n",
    "    update_step_num_gradient_steps_per_sample=1.0,\n",
    "    stop_fn=stop_fn,\n",
    "    logger=logger,\n",
    "    test_in_training=True,\n",
    ")\n",
    "\n",
    "result = algorithm.run_training(off_policy_trainer_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tianshou-td3 (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
