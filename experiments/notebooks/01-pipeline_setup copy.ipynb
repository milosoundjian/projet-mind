{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "598d5ef2",
   "metadata": {},
   "source": [
    "# Pipeline draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3154c762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from functools import partial\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import bbrl_utils\n",
    "from bbrl_utils.notebook import setup_tensorboard\n",
    "from bbrl.stats import WelchTTest\n",
    "from bbrl.agents import Agent, Agents, TemporalAgent\n",
    "from bbrl.agents.gymnasium import ParallelGymAgent, make_env\n",
    "from bbrl.workspace import Workspace\n",
    "from bbrl.utils.replay_buffer import ReplayBuffer\n",
    "\n",
    "import bbrl_gymnasium\n",
    "\n",
    "from pmind.algorithms import DQN, DDPG, TD3\n",
    "from pmind.losses import dqn_compute_critic_loss, ddqn_compute_critic_loss\n",
    "from pmind.training import run_dqn, run_ddpg, run_td3, run_td3_offline\n",
    "from pmind.replay import (\n",
    "    collect_policy_transitions,\n",
    "    collect_uniform_transitions,\n",
    "    collect_uniform_transitions_2,\n",
    "    mix_transitions,\n",
    "    test_rb_compositions,\n",
    ")\n",
    "\n",
    "from pmind.config.loader import load_config\n",
    "\n",
    "bbrl_utils.setup()\n",
    "\n",
    "cfg = load_config(\"bbrl_example\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bf171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup_tensorboard(\"./outputs/tblogs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dd9215",
   "metadata": {},
   "source": [
    "# Obtain transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7939b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = [\n",
    "    \"CartPoleContinuous-v1\",\n",
    "    \"Pendulum-v1\",\n",
    "    \"MountainCarContinuous-v0\",\n",
    "    \"LunarLander-v3\",\n",
    "][0]\n",
    "BUFFER_SIZE = 100_000\n",
    "PROPORTIONS = np.linspace(0, 1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365b5604",
   "metadata": {},
   "source": [
    "## Best policy\n",
    "\n",
    "Get the best policy and get transitions with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6070065",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_td3_best = OmegaConf.create(cfg.td3)\n",
    "\n",
    "# accelerate for the sake of test:\n",
    "cfg_td3_best.algorithm.max_epochs = 11_000\n",
    "cfg_td3_best.algorithm.learning_starts = 1000\n",
    "\n",
    "td3 = TD3(cfg_td3_best)\n",
    "run_td3(td3)\n",
    "td3.visualize_best()\n",
    "best_policy_agent = td3.best_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cb3cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rb_best = collect_policy_transitions(\n",
    "    best_policy_agent, ENV_NAME, buffer_size=BUFFER_SIZE\n",
    ")\n",
    "print(f\"Replay buffer with {rb_best.size()} transitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac7de05",
   "metadata": {},
   "source": [
    "## Uniform exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461386c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rb_unif = collect_uniform_transitions(ENV_NAME, buffer_size=BUFFER_SIZE)\n",
    "rb_unif2 = collect_uniform_transitions_2(ENV_NAME)\n",
    "print(f\"Replay buffer with {rb_unif.size()} transitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fee0c2",
   "metadata": {},
   "source": [
    "Milos: dict_keys(['env/env_obs', 'env/terminated', 'env/truncated', 'env/done', 'env/reward', 'env/cumulated_reward', 'env/timestep', 'action'])\n",
    "\n",
    "Vlad: dict_keys(['env/env_obs', 'action', 'env/timestep', 'env/terminated', 'env/truncated', 'env/done', 'env/reward', 'env/cumulated_reward'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61770ab6",
   "metadata": {},
   "source": [
    "# Offline learning on mixed replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db85183",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_td3_offline = OmegaConf.create(cfg.td3)\n",
    "\n",
    "# accelerate for the sake of test:\n",
    "cfg_td3_offline.algorithm.n_steps = 100\n",
    "cfg_td3_offline.algorithm.max_epochs = 1000\n",
    "\n",
    "# we don't care about when learning starts for offline:\n",
    "cfg_td3_offline.algorithm.learning_starts = None\n",
    "\n",
    "# TODO: apply no noise to actions during learning or it's independent?\n",
    "# cfg_td3_offline.action_noise = 0\n",
    "# cfg_td3_offline.target_policy_noise = 0\n",
    "\n",
    "\n",
    "perfs = test_rb_compositions(\n",
    "    rb_unif=rb_unif2,\n",
    "    rb_best=rb_best,\n",
    "    buffer_size=BUFFER_SIZE,\n",
    "    proportions=PROPORTIONS,\n",
    "    agent_constructor=TD3,\n",
    "    cfg=cfg_td3_offline,\n",
    "    offline_run=run_td3_offline,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projet-mind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
