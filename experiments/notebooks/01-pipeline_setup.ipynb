{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "598d5ef2",
   "metadata": {},
   "source": [
    "# Pipeline draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3154c762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from functools import partial\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import bbrl_utils\n",
    "from bbrl_utils.notebook import setup_tensorboard\n",
    "from bbrl.stats import WelchTTest\n",
    "from bbrl.agents import Agent, Agents, TemporalAgent\n",
    "from bbrl.agents.gymnasium import ParallelGymAgent, make_env\n",
    "from bbrl.workspace import Workspace\n",
    "from bbrl.utils.replay_buffer import ReplayBuffer\n",
    "\n",
    "import bbrl_gymnasium\n",
    "\n",
    "from pmind.algorithms import DQN, DDPG, TD3\n",
    "from pmind.losses import dqn_compute_critic_loss, ddqn_compute_critic_loss\n",
    "from pmind.training import run_dqn, run_ddpg, run_td3, run_td3_offline\n",
    "from pmind.replay import (collect_policy_transitions, \n",
    "                          collect_uniform_transitions, \n",
    "                          collect_uniform_transitions_2,\n",
    "                          mix_transitions, \n",
    "                          test_rb_compositions)\n",
    "\n",
    "from pmind.config.loader import load_config\n",
    "\n",
    "bbrl_utils.setup()\n",
    "\n",
    "cfg = load_config(\"bbrl_example\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bf171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup_tensorboard(\"./outputs/tblogs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dd9215",
   "metadata": {},
   "source": [
    "# Obtain transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7939b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = [\"CartPoleContinuous-v1\",\"Pendulum-v1\",\"MountainCarContinuous-v0\",\"LunarLanderContinuous-v3\"][2]\n",
    "BUFFER_SIZE = 100_000\n",
    "PROPORTIONS = np.linspace(0,1,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365b5604",
   "metadata": {},
   "source": [
    "## Best policy\n",
    "\n",
    "Get the best policy and get transitions with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6070065",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_td3_best = OmegaConf.create(cfg.td3)\n",
    "cfg_td3_best.gym_env.env_name = ENV_NAME\n",
    "\n",
    "# accelerate for the sake of test:\n",
    "cfg_td3_best.algorithm.max_epochs = 11_000\n",
    "cfg_td3_best.algorithm.learning_starts = 1000\n",
    "\n",
    "# playing with parameters for mountain car:\n",
    "# cfg_td3_best.algorithm.action_noise = 0.3\n",
    "# cfg_td3_best.algorithm.discount_factor = 0.995\n",
    "# cfg_td3_best.actor_optimizer.lr = 1e-4\n",
    "# cfg_td3_best.critic_optimizer.lr = 1e-3\n",
    "# cfg_td3_best.algorithm.tau_target = 0.005\n",
    "\n",
    "cfg_td3_best.plot_agents=False # TODO: plot_agents() doesn't work with Lunar Lander\n",
    "\n",
    "td3 = TD3(cfg_td3_best)\n",
    "run_td3(td3)\n",
    "td3.visualize_best()\n",
    "best_policy_agent = td3.best_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cb3cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rb_best = collect_policy_transitions(best_policy_agent ,ENV_NAME, buffer_size=BUFFER_SIZE)\n",
    "print(f\"Replay buffer with {rb_best.size()} transitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac7de05",
   "metadata": {},
   "source": [
    "## Uniform exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461386c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rb_unif = collect_uniform_transitions(ENV_NAME, buffer_size=BUFFER_SIZE)\n",
    "print(f\"Replay buffer with {rb_unif.size()} transitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61770ab6",
   "metadata": {},
   "source": [
    "# Offline learning on mixed replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db85183",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_td3_offline = OmegaConf.create(cfg.td3)\n",
    "cfg_td3_offline.gym_env.env_name = ENV_NAME\n",
    "\n",
    "# accelerate for the sake of test:\n",
    "cfg_td3_offline.algorithm.n_steps = 100\n",
    "cfg_td3_offline.algorithm.max_epochs = 1000\n",
    "\n",
    "# we don't care about when learning starts for offline:\n",
    "cfg_td3_offline.algorithm.learning_starts = None\n",
    "\n",
    "# do we still need to add noise to agent's actions?\n",
    "cfg_td3_offline.algorithm.action_noise = 0.5\n",
    "cfg_td3_offline.algorithm.target_policy_noise = 0.5\n",
    "\n",
    "cfg_td3_offline.plot_agents=False # TODO: plot_agents() doesn't work with Lunar Lander\n",
    "\n",
    "\n",
    "perfs = test_rb_compositions(rb_unif=rb_unif, \n",
    "                    rb_best=rb_best,\n",
    "                    buffer_size=BUFFER_SIZE,\n",
    "                    proportions=[1],#PROPORTIONS,\n",
    "                    agent_constructor=TD3,\n",
    "                    cfg= cfg_td3_offline,\n",
    "                    offline_run=run_td3_offline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projet-mind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
