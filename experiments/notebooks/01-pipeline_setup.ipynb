{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "598d5ef2",
   "metadata": {},
   "source": [
    "# Pipeline draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3154c762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from functools import partial\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "import bbrl_utils\n",
    "from bbrl_utils.notebook import setup_tensorboard\n",
    "from bbrl.stats import WelchTTest\n",
    "from bbrl.agents import Agent, Agents, TemporalAgent\n",
    "from bbrl.agents.gymnasium import ParallelGymAgent, make_env\n",
    "from bbrl.workspace import Workspace\n",
    "from bbrl.utils.replay_buffer import ReplayBuffer\n",
    "\n",
    "import bbrl_gymnasium\n",
    "\n",
    "from pmind.algorithms import DQN, DDPG, TD3, OfflineTD3\n",
    "from pmind.losses import dqn_compute_critic_loss, ddqn_compute_critic_loss\n",
    "from pmind.training import (\n",
    "    run_dqn, \n",
    "    run_ddpg, \n",
    "    run_td3, \n",
    "    run_td3_offline,\n",
    "    load_trained_agents,\n",
    ")\n",
    "from pmind.replay import (\n",
    "    collect_policy_transitions, \n",
    "    collect_uniform_transitions, \n",
    "    mix_transitions, \n",
    "    test_rb_compositions,\n",
    ")\n",
    "\n",
    "from pmind.visualization import (\n",
    "    plot_perf_vs_rb_composition_from_dict\n",
    ")\n",
    "\n",
    "from pmind.config.loader import load_config\n",
    "\n",
    "bbrl_utils.setup()\n",
    "\n",
    "cfg = load_config(\"bbrl_example\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bf171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup_tensorboard(\"./outputs/tblogs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dd9215",
   "metadata": {},
   "source": [
    "# Obtain transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a194f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.registry.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7939b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = (\n",
    "    \"CartPoleContinuous-v1\",\n",
    "    \"Pendulum-v1\",\n",
    "    \"MountainCarContinuous-v0\",\n",
    "    \"LunarLanderContinuous-v3\",\n",
    ")[0]\n",
    "REWARDS = [500] # CartPole: [113, 225, 375, 500]\n",
    "BUFFER_SIZE = 100_000\n",
    "RETRAIN_EXPLOIT = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365b5604",
   "metadata": {},
   "source": [
    "## Best policy\n",
    "\n",
    "Get the best policy and get transitions with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6070065",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN_EXPLOIT:\n",
    "    cfg_td3_exploit = OmegaConf.create(cfg.td3)\n",
    "    cfg_td3_exploit.gym_env.env_name = ENV_NAME\n",
    "\n",
    "    # accelerate for the sake of test:\n",
    "    cfg_td3_exploit.algorithm.max_epochs = 11_000\n",
    "    cfg_td3_exploit.algorithm.learning_starts = 1000\n",
    "\n",
    "    # TODO: plot_agents() doesn't work with Lunar Lander\n",
    "    cfg_td3_exploit.plot_agents=False \n",
    "\n",
    "    # Playing with parameters for mountain car:\n",
    "    # cfg_td3_exploit.algorithm.action_noise = 0.3\n",
    "    # cfg_td3_exploit.algorithm.discount_factor = 0.999\n",
    "    # cfg_td3_exploit.actor_optimizer.lr = 1e-3\n",
    "    # cfg_td3_exploit.critic_optimizer.lr = 1e-3\n",
    "    # cfg_td3_exploit.algorithm.tau_target = 0.005\n",
    "    # cfg_td3_exploit.algorithm.learning_starts = 2_000\n",
    "    # cfg_td3_exploit.algorithm.architecture.critic_hidden_size = [128,128]\n",
    "    # cfg_td3_exploit.algorithm.architecture.actor_hidden_size = [128,128]\n",
    "    \n",
    "\n",
    "    td3 = TD3(cfg_td3_exploit)\n",
    "    trained_agents = run_td3(td3, save_model_at_rewards=REWARDS)\n",
    "    td3.visualize_best()\n",
    "else:\n",
    "    trained_agents = load_trained_agents(ENV_NAME, REWARDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cb3cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rb_exploit_by_level = {}\n",
    "for k, policy_agent in trained_agents.items():\n",
    "    # TODO: save rbs!\n",
    "    rb_exploit_by_level[k] = collect_policy_transitions(policy_agent, ENV_NAME, BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac7de05",
   "metadata": {},
   "source": [
    "## Uniform exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461386c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rb_unif = collect_uniform_transitions(ENV_NAME, buffer_size=BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61770ab6",
   "metadata": {},
   "source": [
    "# Offline learning on mixed replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ada7d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROPORTIONS = np.linspace(0,1,10)\n",
    "SEEDS = [0] # [0,1,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db85183",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_td3_offline = OmegaConf.create(cfg.td3)\n",
    "cfg_td3_offline.gym_env.env_name = ENV_NAME\n",
    "\n",
    "# to get 100K training steps:\n",
    "cfg_td3_offline.algorithm.n_steps = 100\n",
    "cfg_td3_offline.algorithm.max_epochs = 1000 # TODO: epochs don't actually make sense for offline\n",
    "\n",
    "# learning starts immediately for offline:\n",
    "cfg_td3_offline.algorithm.learning_starts = None\n",
    "\n",
    "# there is no exploration in offline learning \n",
    "cfg_td3_offline.algorithm.action_noise = None\n",
    "cfg_td3_offline.algorithm.target_policy_noise = None\n",
    "\n",
    "cfg_td3_offline.plot_agents=False # TODO: plot_agents() doesn't work with Lunar Lander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876d3a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACCELERATE FOR THE TEST: \n",
    "# cfg_td3_offline.algorithm.n_steps = 10\n",
    "# cfg_td3_offline.algorithm.max_epochs = 10\n",
    "# cfg_td3_offline.algorithm.eval_interval = 2\n",
    "# cfg_td3_offline.algorithm.nb_evals= 5\n",
    "\n",
    "\n",
    "performances = {}\n",
    "for k, rb_exploit in rb_exploit_by_level.items():\n",
    "    # TODO: find a way to temporarily silence tqdm\n",
    "    performances[k] = test_rb_compositions(\n",
    "        rb_unif=rb_unif,\n",
    "        rb_exploit=rb_exploit,\n",
    "        buffer_size=BUFFER_SIZE,\n",
    "        proportions= PROPORTIONS,\n",
    "        agent_constructor=OfflineTD3,\n",
    "        cfg=cfg_td3_offline,\n",
    "        offline_run=run_td3_offline,\n",
    "        seeds=SEEDS # TODO: make buffer mixing take this seed as well\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9e2dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_perf_vs_rb_composition_from_dict(\n",
    "    PROPORTIONS, \n",
    "    performances,\n",
    "    last_n=1, # NOTE: depends on evaluation interval\n",
    "    legend_title=\"Exploitation Reward\",\n",
    "    fig_name=f\"{ENV_NAME}-offline-diff-best-policies-{int(time.time())}.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projet-mind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
