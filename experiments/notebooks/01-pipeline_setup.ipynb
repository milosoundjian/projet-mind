{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "598d5ef2",
   "metadata": {},
   "source": [
    "# Pipeline draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3154c762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from functools import partial\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import bbrl_utils\n",
    "from bbrl_utils.notebook import setup_tensorboard\n",
    "from bbrl.stats import WelchTTest\n",
    "from bbrl.agents import Agent, Agents, TemporalAgent\n",
    "from bbrl.agents.gymnasium import ParallelGymAgent, make_env\n",
    "from bbrl.workspace import Workspace\n",
    "from bbrl.utils.replay_buffer import ReplayBuffer\n",
    "\n",
    "import bbrl_gymnasium\n",
    "\n",
    "from pmind.algorithms import DQN, DDPG, TD3\n",
    "from pmind.losses import dqn_compute_critic_loss, ddqn_compute_critic_loss\n",
    "from pmind.training import run_dqn, run_ddpg, run_td3, run_td3_offline\n",
    "from pmind.replay import get_gym_agent, get_workspace, mix_transitions, test_rb_compositions\n",
    "\n",
    "from pmind.config.loader import load_config\n",
    "\n",
    "bbrl_utils.setup()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c33b30",
   "metadata": {},
   "source": [
    "Load all configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24491ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = load_config(\"bbrl_example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bf171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_tensorboard(\"./outputs/tblogs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365b5604",
   "metadata": {},
   "source": [
    "## Best policy:\n",
    "\n",
    "Get the best policy (to eventually exploit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6070065",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_td3_best = OmegaConf.create(cfg.td3)\n",
    "\n",
    "# accelerate for the sake of test:\n",
    "cfg_td3_best.algorithm.max_epochs = 11_000\n",
    "cfg_td3_best.algorithm.learning_starts = 1000\n",
    "\n",
    "td3 = TD3(cfg_td3_best)\n",
    "run_td3(td3)\n",
    "td3.visualize_best()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefedaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_policy_agent = td3.best_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cb3cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_agent = get_gym_agent('CartPoleContinuous-v1', num_envs=10, seed=42)\n",
    "workspace_best = get_workspace(best_policy_agent ,gym_agent, epoch_size=100_000)\n",
    "print(workspace_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac7de05",
   "metadata": {},
   "source": [
    "## Uniform policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc84d1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniformAgent(Agent):\n",
    "    '''Agent that explores uniformly a given environment'''\n",
    "\n",
    "    # TODO: for now it just does a random walk,\n",
    "    #   need to do jumps instead - random actions in random states\n",
    "    # I think need to modify ParallelGymAgent._reset() method\n",
    "    def __init__(self, env_name):\n",
    "        super().__init__()\n",
    "        self.env = gym.make(env_name) \n",
    "\n",
    "    def forward(self, t: int):\n",
    "        \"\"\"An Agent can use self.workspace\"\"\"\n",
    "        # obs = self.get((\"env/env_obs\", t))\n",
    "        n_env = self.workspace.batch_size()\n",
    "        action = torch.tensor([self.env.action_space.sample() for _ in range(n_env) ], dtype=torch.float32)\n",
    "        self.set((\"action\", t), action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d19360",
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace_unif = get_workspace(UniformAgent('CartPoleContinuous-v1'), gym_agent, epoch_size=100_000)\n",
    "print(workspace_unif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40992172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poking around with the idea: may be it's easier to implement those \n",
    "# random jumps as episodes of length 2?\n",
    "# for k in range(gym_agent.num_envs):\n",
    "#     env = gym_agent.envs[k]\n",
    "#     env.reset()\n",
    "#     env.state = env.unwrapped.state = env.observation_space.sample()\n",
    "# gym_agent = get_gym_agent('CartPoleContinuous-v1', num_envs=3, seed=42)\n",
    "# t_agents = TemporalAgent(Agents(gym_agent,UniformAgent('CartPoleContinuous-v1')))\n",
    "# workspace = Workspace()\n",
    "# t_agents(workspace, t=1,n_steps=2)\n",
    "# workspace_unif.get_transitions()[\"env/reward\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66127fc8",
   "metadata": {},
   "source": [
    "## Mix transitions in a buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ba4c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "rb_mixed = mix_transitions(workspace_unif,\n",
    "                           workspace_best,\n",
    "                           buffer_size=100_000, \n",
    "                           proportion=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61770ab6",
   "metadata": {},
   "source": [
    "# Offline learning on mixed replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db85183",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_td3_offline = OmegaConf.create(cfg.td3)\n",
    "\n",
    "# accelerate for the sake of test:\n",
    "cfg_td3_offline.algorithm.n_steps = 100\n",
    "cfg_td3_offline.algorithm.max_epochs = 1000\n",
    "\n",
    "# we don't care about when learning starts for offline:\n",
    "cfg_td3_offline.algorithm.learning_starts = None\n",
    "\n",
    "cfg_td3_offline.action_noise = 0\n",
    "cfg_td3_offline.target_policy_noise = 0\n",
    "\n",
    "td3_offline = TD3(cfg_td3_offline)\n",
    "run_td3_offline(td3_offline, rb_mixed)\n",
    "td3_offline.visualize_best()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5f9440",
   "metadata": {},
   "outputs": [],
   "source": [
    "perfs = test_rb_compositions(workspace_unif=workspace_unif, \n",
    "                    workspace_best=workspace_best,\n",
    "                    buffer_size=100_000,\n",
    "                    proportions=np.linspace(0,1,10),\n",
    "                    agent_constructor=TD3,\n",
    "                    cfg= cfg_td3_offline,\n",
    "                    offline_run=run_td3_offline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projet-mind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
